{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ładowanie strony 1/1\n",
      "Pobieranie informacji ze strony 1/32\n",
      "Pobieranie informacji ze strony 2/32\n",
      "Pobieranie informacji ze strony 3/32\n",
      "Pobieranie informacji ze strony 4/32\n",
      "Pobieranie informacji ze strony 5/32\n",
      "Pobieranie informacji ze strony 6/32\n",
      "Pobieranie informacji ze strony 7/32\n",
      "Pobieranie informacji ze strony 8/32\n",
      "Pobieranie informacji ze strony 9/32\n",
      "Pobieranie informacji ze strony 10/32\n",
      "Pobieranie informacji ze strony 11/32\n",
      "Pobieranie informacji ze strony 12/32\n",
      "Pobieranie informacji ze strony 13/32\n",
      "Pobieranie informacji ze strony 14/32\n",
      "Pobieranie informacji ze strony 15/32\n",
      "Pobieranie informacji ze strony 16/32\n",
      "Pobieranie informacji ze strony 17/32\n",
      "Pobieranie informacji ze strony 18/32\n",
      "Pobieranie informacji ze strony 19/32\n",
      "Pobieranie informacji ze strony 20/32\n",
      "Pobieranie informacji ze strony 21/32\n",
      "Pobieranie informacji ze strony 22/32\n",
      "Pobieranie informacji ze strony 23/32\n",
      "Pobieranie informacji ze strony 24/32\n",
      "Pobieranie informacji ze strony 25/32\n",
      "Pobieranie informacji ze strony 26/32\n",
      "Pobieranie informacji ze strony 27/32\n",
      "Pobieranie informacji ze strony 28/32\n",
      "Pobieranie informacji ze strony 29/32\n",
      "Pobieranie informacji ze strony 30/32\n",
      "Pobieranie informacji ze strony 31/32\n",
      "Pobieranie informacji ze strony 32/32\n",
      "Zapisane do output1.plik csv\n",
      "Zapisane do output1.plik json\n",
      "aktualizacja tabeli... gratka.\n",
      "dane output1.csv przesłane do tablicy BigQuery gratkaproject.gratka.gratka.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import dataplex\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def getInfoFromPage(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    paraminfo = soup.find_all(\"b\", \"parameters__value\")\n",
    "\n",
    "    title = soup.find(\"h1\", \"sticker__title\")\n",
    "    if (title != None): title = title.text.strip()\n",
    "    else: title = \"NULL\"\n",
    "\n",
    "    if (len(paraminfo) >= 1):\n",
    "        city = re.sub(\" +\", \" \", paraminfo[0].text.strip())\n",
    "    else: city = \"NULL\"\n",
    "\n",
    "    if (len(paraminfo) >= 2):\n",
    "        square = paraminfo[1].text.strip()\n",
    "    else: square = \"NULL\"\n",
    "\n",
    "    if (len(paraminfo) >= 3):\n",
    "        rooms = paraminfo[2].text.strip()\n",
    "    else: rooms = \"NULL\"\n",
    "\n",
    "    if (len(paraminfo) >= 4):\n",
    "        floor = paraminfo[3].text.strip()\n",
    "    else: floor = \"NULL\"\n",
    "\n",
    "    if (len(paraminfo) >= 5):\n",
    "        for text in paraminfo:\n",
    "            builtIn = re.findall(\"(?:[1][9][0-9][0-9]|[2][0][0-2][0-9])\", text.text)\n",
    "            if (builtIn == []): builtIn = \"NULL\"\n",
    "            else:\n",
    "                builtIn = builtIn[0]\n",
    "                break\n",
    "    else: builtIn = \"NULL\"\n",
    "\n",
    "    paraminfo = soup.find_all(\"div\", \"parameters__value\")\n",
    "\n",
    "    if (len(paraminfo) >= 1):\n",
    "        updateTime = paraminfo[0].text.strip()\n",
    "    else: updateTime = \"NULL\"\n",
    "\n",
    "    if (len(paraminfo) >= 1):\n",
    "        uploadTime = paraminfo[1].text.strip()\n",
    "    else: uploadTime = \"NULL\"\n",
    "\n",
    "    price = soup.find(\"span\", \"priceInfo__value\")\n",
    "    if (price != None): price = re.sub(\" +\", \" \", soup.find(\"span\", \"priceInfo__value\").text.replace(\"\\n\", '').strip())\n",
    "    else: price = \"NULL\"\n",
    "\n",
    "    pricePerMeter = soup.find(\"span\", \"priceInfo__additional\")\n",
    "    if (pricePerMeter != None): pricePerMeter = pricePerMeter.text.strip()\n",
    "    else: pricePerMeter = \"NULL\"\n",
    "\n",
    "    description = soup.find(\"div\", \"description__container\")\n",
    "    if (description != None): description = description.text.replace(\"\\n\", '').replace(\"\\r\", '').strip()\n",
    "    else: description = \"NULL\"\n",
    "\n",
    "    return (title, city, square, rooms, floor, builtIn, updateTime, uploadTime, price, pricePerMeter, description, url)\n",
    "\n",
    "def getLinksFromPage(url):\n",
    "    links = []\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    divs = soup.find_all(\"div\", \"listing__teaserWrapper\")\n",
    "    for div in divs:\n",
    "        links.append(div.find('a', \"teaserLink\")['href'])\n",
    "    return links\n",
    "\n",
    "def getMaxPage(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    return soup.find('a', \"\")\n",
    "\n",
    "df = pd.DataFrame(columns=['Title', 'City', 'Square', 'Rooms', 'Floor', 'Built In', 'Update time', 'Upload time', 'Price',\n",
    "                           'Price per meter', 'Description', 'Link'])\n",
    "\n",
    "links = []\n",
    "URL = \"https://gratka.pl/nieruchomosci/mieszkania?page=\"\n",
    "MaxPage = 2\n",
    "\n",
    "for i in range(1, MaxPage):\n",
    "    links = links + getLinksFromPage(URL + str(i))\n",
    "    print(\"Ładowanie strony \" + str(i) + \"/\" + str(MaxPage - 1))\n",
    "\n",
    "for n, link in enumerate(links):\n",
    "    df.loc[n] = getInfoFromPage(link)\n",
    "    print(\"Pobieranie informacji ze strony \" + str(n + 1) + \"/\" + str(len(links)))\n",
    "\n",
    "# Сохранение в CSV\n",
    "df.to_csv(r\"output1.csv\", index=True, header=True)\n",
    "print(\"Zapisane do output1.plik csv\")\n",
    "\n",
    "# Преобразование в JSON\n",
    "df_json = df.to_json(orient='records')\n",
    "with open('output1.json', 'w') as json_file:\n",
    "    json_file.write(df_json)\n",
    "print(\"Zapisane do output1.plik json\")\n",
    "\n",
    "\n",
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(f'File {source_file_name} uploaded to {destination_blob_name} in {bucket_name}.')\n",
    "\n",
    "source_file_name = 'output1.csv'\n",
    "\n",
    "#############\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'key.json'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket_name = 'gratka_bucket'\n",
    "\n",
    "# Проверяем, существует ли ведро\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "if not bucket.exists():\n",
    "    bucket = storage_client.create_bucket(bucket, location='EU')\n",
    "\n",
    "def upload_to_bucket(blob_name, file_path, bucket_name):\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(file_path)\n",
    "    return True\n",
    "\n",
    "\n",
    "file_path = r'C:\\Users\\lukum\\PycharmProjects\\pythonProject\\output1.csv'\n",
    "upload_to_bucket('output1.csv', file_path, 'gratka_bucket')\n",
    "\n",
    "def clear_table(client, dataset_name, table_name):\n",
    "    \"\"\"Удаляет все строки из таблицы BigQuery.\"\"\"\n",
    "    dataset_ref = client.dataset(dataset_name)\n",
    "    table_ref = dataset_ref.table(table_name)\n",
    "\n",
    "    # Получаем объект таблицы\n",
    "    table = client.get_table(table_ref)\n",
    "\n",
    "    # Формируем SQL-запрос для очистки таблицы (используя TRUNCATE TABLE)\n",
    "    sql = f\"TRUNCATE TABLE `{table.project}.{table.dataset_id}.{table.table_id}`\"\n",
    "\n",
    "    # Выполняем запрос\n",
    "    query_job = client.query(sql)\n",
    "    query_job.result()  # Ждем завершения запроса\n",
    "\n",
    "    print(f\"aktualizacja tabeli... {table.table_id}.\")\n",
    "\n",
    "def upload_to_bigquery(bucket_name, source_file_name, dataset_name, table_name):\n",
    "    \"\"\"Загружает CSV-файл из Cloud Storage в таблицу BigQuery.\"\"\"\n",
    "    # Создаем экземпляр клиента BigQuery\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Очищаем таблицу\n",
    "    clear_table(client, dataset_name, table_name)\n",
    "\n",
    "    # Определяем URI источника и таблицы назначения\n",
    "    source_uri = f\"gs://{bucket_name}/{source_file_name}\"\n",
    "    destination_table = f\"{client.project}.{dataset_name}.{table_name}\"\n",
    "\n",
    "    # Загружаем файл в таблицу BigQuery\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1,  # Пропускаем заголовок\n",
    "        field_delimiter=',',  # Можно изменить разделитель полей, если он отличается\n",
    "    )\n",
    "    load_job = client.load_table_from_uri(\n",
    "        source_uri,\n",
    "        destination_table,\n",
    "        job_config=job_config\n",
    "    )\n",
    "\n",
    "    # Ожидаем завершения загрузки\n",
    "    load_job.result()\n",
    "\n",
    "    print(f\"dane {source_file_name} przesłane do tablicy BigQuery {destination_table}.\")\n",
    "\n",
    "# Пример использования:\n",
    "upload_to_bigquery(bucket_name, \"output1.csv\", \"gratka\", \"gratka\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
