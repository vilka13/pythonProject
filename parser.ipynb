{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page 1/1\n",
      "Getting info from page 1/32\n",
      "Getting info from page 2/32\n",
      "Getting info from page 3/32\n",
      "Getting info from page 4/32\n",
      "Getting info from page 5/32\n",
      "Getting info from page 6/32\n",
      "Getting info from page 7/32\n",
      "Getting info from page 8/32\n",
      "Getting info from page 9/32\n",
      "Getting info from page 10/32\n",
      "Getting info from page 11/32\n",
      "Getting info from page 12/32\n",
      "Getting info from page 13/32\n",
      "Getting info from page 14/32\n",
      "Getting info from page 15/32\n",
      "Getting info from page 16/32\n",
      "Getting info from page 17/32\n",
      "Getting info from page 18/32\n",
      "Getting info from page 19/32\n",
      "Getting info from page 20/32\n",
      "Getting info from page 21/32\n",
      "Getting info from page 22/32\n",
      "Getting info from page 23/32\n",
      "Getting info from page 24/32\n",
      "Getting info from page 25/32\n",
      "Getting info from page 26/32\n",
      "Getting info from page 27/32\n",
      "Getting info from page 28/32\n",
      "Getting info from page 29/32\n",
      "Getting info from page 30/32\n",
      "Getting info from page 31/32\n",
      "Getting info from page 32/32\n",
      "Saved to output1.csv file\n",
      "Saved to output1.json file\n",
      "Данные из файла output1.csv загружены в таблицу BigQuery gratkaproject.gratka.gratka.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import dataplex\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getInfoFromPage(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    paraminfo = soup.find_all(\"b\", \"parameters__value\")\n",
    "\n",
    "    title = soup.find(\"h1\", \"sticker__title\")\n",
    "    if (title != None): title = title.text.strip()\n",
    "    else: title = \"NULL\"\n",
    "\n",
    "    if (len(paraminfo) >= 1):\n",
    "        city = re.sub(\" +\", \" \", paraminfo[0].text.strip())\n",
    "    else: city = \"NULL\"\n",
    "\n",
    "    if (len(paraminfo) >= 2):\n",
    "        square = paraminfo[1].text.strip()\n",
    "    else: square = \"NULL\"\n",
    "\n",
    "    if (len(paraminfo) >= 3):\n",
    "        rooms = paraminfo[2].text.strip()\n",
    "    else: rooms = \"NULL\"\n",
    "\n",
    "    if (len(paraminfo) >= 4):\n",
    "        floor = paraminfo[3].text.strip()\n",
    "    else: floor = \"NULL\"\n",
    "\n",
    "    if (len(paraminfo) >= 5):\n",
    "        for text in paraminfo:\n",
    "            builtIn = re.findall(\"(?:[1][9][0-9][0-9]|[2][0][0-2][0-9])\", text.text)\n",
    "            if (builtIn == []): builtIn = \"NULL\"\n",
    "            else:\n",
    "                builtIn = builtIn[0]\n",
    "                break\n",
    "    else: builtIn = \"NULL\"\n",
    "\n",
    "    paraminfo = soup.find_all(\"div\", \"parameters__value\")\n",
    "\n",
    "    if (len(paraminfo) >= 1):\n",
    "        updateTime = paraminfo[0].text.strip()\n",
    "    else: updateTime = \"NULL\"\n",
    "\n",
    "    if (len(paraminfo) >= 1):\n",
    "        uploadTime = paraminfo[1].text.strip()\n",
    "    else: uploadTime = \"NULL\"\n",
    "\n",
    "    price = soup.find(\"span\", \"priceInfo__value\")\n",
    "    if (price != None): price = re.sub(\" +\", \" \", soup.find(\"span\", \"priceInfo__value\").text.replace(\"\\n\", '').strip())\n",
    "    else: price = \"NULL\"\n",
    "\n",
    "    pricePerMeter = soup.find(\"span\", \"priceInfo__additional\")\n",
    "    if (pricePerMeter != None): pricePerMeter = pricePerMeter.text.strip()\n",
    "    else: pricePerMeter = \"NULL\"\n",
    "\n",
    "    description = soup.find(\"div\", \"description__container\")\n",
    "    if (description != None): description = description.text.replace(\"\\n\", '').replace(\"\\r\", '').strip()\n",
    "    else: description = \"NULL\"\n",
    "\n",
    "    return (title, city, square, rooms, floor, builtIn, updateTime, uploadTime, price, pricePerMeter, description, url)\n",
    "\n",
    "def getLinksFromPage(url):\n",
    "    links = []\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    divs = soup.find_all(\"div\", \"listing__teaserWrapper\")\n",
    "    for div in divs:\n",
    "        links.append(div.find('a', \"teaserLink\")['href'])\n",
    "    return links\n",
    "\n",
    "def getMaxPage(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    return soup.find('a', \"\")\n",
    "\n",
    "df = pd.DataFrame(columns=['Title', 'City', 'Square', 'Rooms', 'Floor', 'Built In', 'Update time', 'Upload time', 'Price',\n",
    "                           'Price per meter', 'Description', 'Link'])\n",
    "\n",
    "links = []\n",
    "URL = \"https://gratka.pl/nieruchomosci/mieszkania?page=\"\n",
    "MaxPage = 2\n",
    "\n",
    "for i in range(1, MaxPage):\n",
    "    links = links + getLinksFromPage(URL + str(i))\n",
    "    print(\"Loading page \" + str(i) + \"/\" + str(MaxPage - 1))\n",
    "\n",
    "for n, link in enumerate(links):\n",
    "    df.loc[n] = getInfoFromPage(link)\n",
    "    print(\"Getting info from page \" + str(n + 1) + \"/\" + str(len(links)))\n",
    "\n",
    "# Сохранение в CSV\n",
    "df.to_csv(r\"output1.csv\", index=True, header=True)\n",
    "print(\"Saved to output1.csv file\")\n",
    "\n",
    "# Преобразование в JSON\n",
    "df_json = df.to_json(orient='records')\n",
    "with open('output1.json', 'w') as json_file:\n",
    "    json_file.write(df_json)\n",
    "print(\"Saved to output1.json file\")\n",
    "\n",
    "\n",
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(f'File {source_file_name} uploaded to {destination_blob_name} in {bucket_name}.')\n",
    "\n",
    "source_file_name = 'output1.csv'\n",
    "\n",
    "#############\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'key.json'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket_name = 'gratka_bucket'\n",
    "\n",
    "# Проверяем, существует ли ведро\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "if not bucket.exists():\n",
    "    bucket = storage_client.create_bucket(bucket, location='EU')\n",
    "\n",
    "def upload_to_bucket(blob_name, file_path, bucket_name):\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(file_path)\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_path = r'C:\\Users\\lukum\\PycharmProjects\\pythonProject\\output1.csv'\n",
    "upload_to_bucket('output1.csv', file_path, 'gratka_bucket')\n",
    "\n",
    "def upload_to_bigquery(bucket_name, source_file_name, dataset_name, table_name):\n",
    "    \"\"\"Загружает CSV-файл из Cloud Storage в таблицу BigQuery.\"\"\"\n",
    "    # Создаем экземпляр клиента BigQuery\n",
    "    client = bigquery.Client()\n",
    "    dataset_name = 'gratka'\n",
    "    table_name = 'gratka'\n",
    "\n",
    "    # Определяем URI источника и таблицы назначения\n",
    "    source_uri = f\"gs://{bucket_name}/{source_file_name}\"\n",
    "    destination_table = f\"{client.project}.{dataset_name}.{table_name}\"\n",
    "\n",
    "    # Загружаем файл в таблицу BigQuery\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1,  # Пропускаем заголовок\n",
    "        field_delimiter=',',  # Можно изменить разделитель полей, если он отличается\n",
    "    )\n",
    "    load_job = client.load_table_from_uri(\n",
    "        source_uri,\n",
    "        destination_table,\n",
    "        job_config=job_config\n",
    "    )\n",
    "\n",
    "    # Ожидаем завершения загрузки\n",
    "    load_job.result()\n",
    "\n",
    "    print(f\"Данные из файла {source_file_name} загружены в таблицу BigQuery {destination_table}.\")\n",
    "\n",
    "# Пример использования:\n",
    "upload_to_bigquery(bucket_name, \"output1.csv\", \"gratka\", \"gratka\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
